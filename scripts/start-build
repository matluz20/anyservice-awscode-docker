#!/bin/bash
set -e

if [ "${VERBOSE}" = true ]; then
  set -x
fi

OPTS=$@

# Check for required env vars
if [ -z "${CODEBUILD_S3_BUCKET}" ]; then
  echo "CODEBUILD_S3_BUCKET must be set"
  missing_vars=true
fi

if [ -z "${CODEBUILD_S3_ARCHIVE_KEY}" ]; then
  echo "CODEBUILD_S3_ARCHIVE_KEY must be set"
  missing_vars=true
fi

if [ "${missing_vars}" = "true" ]; then
  exit 1
fi

# Switching AWS principal
if [ -n "${AWS_ASSUME_ROLE}" ]; then
  AWS_ASSUME_ROLE_ARN=${AWS_ASSUME_ROLE}
  unset AWS_ASSUME_ROLE
  # set defaut aws assume-role session duration to 1 hour if not set
  if [ -z "${AWS_ASSUME_ROLE_DURATION}" ]; then
    AWS_ASSUME_ROLE_DURATION=3600
  fi
  . aws_assume_role "${AWS_ASSUME_ROLE_ARN}" --duration-seconds ${AWS_ASSUME_ROLE_DURATION}
fi

# Detect CI environment: either BITBUCKET or GITLAB_CI
if [ -n "${BITBUCKET_COMMIT}" ]; then
  # BITBUCKET doc: https://confluence.atlassian.com/bitbucket/environment-variables-794502608.html
  ci_env_pattern="BITBUCKET_"
  codebuild_result_file="${BITBUCKET_REPO_OWNER}.${BITBUCKET_REPO_SLUG}.${BITBUCKET_COMMIT}.json";

  echo "Detected BitBucket environment"
elif [ -n "${GITLAB_CI}" ]; then
  # GITLAB doc: https://docs.gitlab.com/ce/ci/variables/README.html
  ci_env_pattern="CI_|GITLAB_"
  codebuild_result_file="${CI_PROJECT_NAMESPACE}.${CI_COMMIT_REF_SLUG}.${CI_COMMIT_SHA}.json";

  echo "Detected GitLab environment"
fi

# Add additionals environment variables patterns to pass vars to codebuild when matching a given prefix
if [ -n "${CI_ENV_PATTERN}" ]; then
  ci_env_pattern="${ci_env_pattern}|${CI_ENV_PATTERN}"
fi

# Wait for Codebuild by default
WAIT_FOR_CODEBUILD=${WAIT_FOR_CODEBUILD:-true}

# Build source code archive for selected commit using option -y to preserve symlinks
CODEBUILD_CHROOT=${CODEBUILD_CHROOT:-.}
PWD=`pwd`
cd ${CODEBUILD_CHROOT}
zip -r -y -q ${PWD}/code.zip .
cd ${PWD}

s3_put_archive() {
  aws s3api put-object \
    --body ./code.zip \
    --bucket ${CODEBUILD_S3_BUCKET} \
    --key ${CODEBUILD_S3_ARCHIVE_KEY} \
    --output text --query 'VersionId'
};

## Put source code archive on specified S3 Bucket/Key and extract object version
echo
echo "-> Uploading code (${CODEBUILD_CHROOT}) to S3 location: s3://${CODEBUILD_S3_BUCKET}/${CODEBUILD_S3_ARCHIVE_KEY}"

version_id=$(s3_put_archive)

echo "S3 Object version for uploaded code is: ${version_id}"


# helper to format env vars containing the ci_env_pattern (BITBUCKET_ or CI_|GITLAB_CI_) in a space-separated list of `name=NAME,value=VALUE` elements
ci_env() {
  env \
  | grep -E ${ci_env_pattern} \
  | awk -F= '{print "name="$1 ",value="$2}' \
  | tr "\n" " "
};

# helper to format env vars containing the ci_env_pattern (BITBUCKET_ or CI_|GITLAB_CI_) in a line-separated list `{"name":"NAME","value":"VALUE"}` elements
ci_env_json() {
  env \
    | grep -E ${ci_env_pattern} \
    | awk -F= '{"jq -n -c '\''.name=\"" $1 "\" | .value=\"" $2 "\"'\''" | getline ts; print ts}'
};


## Start build
start_build() {
  ## build a codebuild "cli-input-json" object out of the CI env vars, so they can be passed to codebuild runner
  # format:
  # {
  #   "environmentVariablesOverride": [
  #     {"name":"foo","value":"bar"},
  #     {"name":"bar","value":"baz"},
  #      ...
  #   ]
  # }
  echo $(ci_env_json) | jq -s -c '{environmentVariablesOverride:.}' > /tmp/_cli_input.json;

  # User provided a a codebuild "cli-input-json" file: merge it with environment variables gathered in _cli_input.json file
  # The spec of the user provided cli-input-file can be found here: https://docs.aws.amazon.com/codebuild/latest/APIReference/API_StartBuild.html
  if [ -n "${CODEBUILD_START_JSON_FILE}" ]; then
    jq -s -c '[.[] | to_entries] | flatten | reduce (.[]) as $dot ({}; (if ($dot.value|type == "array") then .[$dot.key] += $dot.value else .[$dot.key] = $dot.value end))' /tmp/_cli_input.json ${CODEBUILD_START_JSON_FILE} > /tmp/_cli_input.json;
  fi

  # make sure the cli input file does not have duplicate env var entries
  jq -s -c '.[] | .environmentVariablesOverride |= unique' /tmp/_cli_input.json > /tmp/cli_input.json;

  # helper to conditionally pass the `--project-name` option to codebuild start-build command
  local project_name_arg="--project-name ${CODEBUILD_PROJECT_NAME}";
  if [ -z ${CODEBUILD_PROJECT_NAME} ]; then
    project_name_arg="";
  fi

  aws codebuild start-build ${project_name_arg} \
    --source-version ${version_id} \
    --cli-input-json file:///tmp/cli_input.json \
    --output text \
    --query 'build.id' \
    ${OPTS}
};

# Start build and record the build id
start_build_id=$(start_build);
echo
echo "-> Started build: ${start_build_id}";
echo "Build details are available at the following url \nhttps://${AWS_DEFAULT_REGION}.console.aws.amazon.com/codebuild/home?region=${AWS_DEFAULT_REGION}#/builds/${start_build_id}/view/new"
echo
## Wait for build result (conditional)
if [ ${WAIT_FOR_CODEBUILD} = "true" ]; then

  fetch_logs() {
    # get cloudwatch logs info
    build_logs=$(aws codebuild batch-get-builds --ids ${start_build_id} --query 'builds[0].logs')
    build_cwlogs_group=`echo ${build_logs} | jq -c -r '.groupName'`;
    build_cwlogs_stream=`echo ${build_logs} | jq -c -r '.streamName'`;

    if [ "${build_cwlogs_group}" = "null" ] || [ "${build_cwlogs_stream}" = "null" ]; then
      echo "Waiting for build phase to start"
      return
    fi

    # get log events
    if [ "${nextForwardToken}" = "" ]; then
      log_events=$(aws logs get-log-events --log-group-name ${build_cwlogs_group} --log-stream-name ${build_cwlogs_stream} --start-from-head)
    else
      log_events=$(aws logs get-log-events --log-group-name ${build_cwlogs_group} --log-stream-name ${build_cwlogs_stream} --start-from-head --next-token ${nextForwardToken})
    fi

    # keep previous nextForwardToken value
    previousNextForwardToken=${nextForwardToken}

    nextForwardToken=`echo ${log_events} | jq -c -r '.nextForwardToken'`;
    events=`echo ${log_events} | jq -c -r '.events'`;

    if [ "$(echo "${events}" | jq -r '.[]')" = "" ] && [ -z ${build_has_started} ]; then
      echo "Waiting for build phase to start"
      return
    elif [ -z ${build_has_started} ]; then
      echo
      echo "Cloudwatch logs are available at the following url \nhttps://${AWS_DEFAULT_REGION}.console.aws.amazon.com/cloudwatch/home?region=${AWS_DEFAULT_REGION}#logEventViewer:group=${build_cwlogs_group};stream=${build_cwlogs_stream}"
      echo
      echo "-> Build phase started, log events follow:"
      echo "------------------------------------------"
      echo
      build_has_started=true
    fi

    # iterate on log events to display log messages only
    # Note: base64 -d on Alpine | --decode on MacOS for `base64 decode`
    for row in $(echo "${events}" | jq -r '.[] | @base64'); do
      echo ${row} | base64 -d | jq -j '.message'
    done

    # if we have reached the end of the stream, get-log-events will return the same token we passed in,
    # meaning that we need to fetch remaining logs at once if we have not reached the end of the stream
    if [ "${nextForwardToken}" != "" ] && [ "${nextForwardToken}" != "${previousNextForwardToken}" ]; then
      fetch_logs
    fi
  }

  ## Wait until build is complete and fetch logs
  until [ "$(aws codebuild batch-get-builds --ids ${start_build_id} --output text --query 'builds[0].buildComplete')" = "True" ];
  do
    fetch_logs;
    # wait 10s before checking build status again
    sleep 10;
  done

  # fetch last logs
  fetch_logs

  ## Build has completed, it is either in state SUCCEEDED or in faulty state: FAILED, FAULT, STOPPED, TIMED_OUT

  # Get build result
  # output spec: https://docs.aws.amazon.com/cli/latest/reference/codebuild/batch-get-builds.html#output
  build_result=$(aws codebuild batch-get-builds --ids "${start_build_id}" --query "builds[0]");

  # Save build result to S3 bucket (conditional)
  if [ -n "$CODEBUILD_S3_RESULT_PATH" ]; then
    echo
    echo "Saving build result to S3 location: s3://${CODEBUILD_S3_BUCKET}/${CODEBUILD_S3_RESULT_PATH}/${codebuild_result_file}"
    echo ${build_result} | jq . > _build_result;

    s3_put_result() {
      aws s3api put-object \
        --body _build_result \
        --bucket ${CODEBUILD_S3_BUCKET} \
        --key "${CODEBUILD_S3_RESULT_PATH}/${codebuild_result_file}"
    }

    quiet=$(s3_put_result)
  fi

  # Extract build status from job details
  codebuild_status=`echo ${build_result} | jq -c -r '.buildStatus'`;

  # Build succeeded
  echo
  if [ ${codebuild_status} = "SUCCEEDED" ]; then
    echo "-> Build ${start_build_id} completed successfully!"

  # Build failed
  else
    echo "-> Build ${start_build_id} failed!"
    echo "Build failure reason: ${codebuild_status}"
    exit 1;
  fi

  # Artifacts
  artifacts=`echo ${build_result} | jq -c -r '.artifacts'`;
  artifacts_location=`echo ${artifacts} | jq -c -r '.location'`;
  
  if [ "${artifacts_location}" != "" ]; then
    echo
    echo "-> Artifacts were generated for this build:"
    echo
    echo "${artifacts}" | jq .
    
    ## Extract artifacts S3 bucket and key
    # Artifacts_location format : "arn:aws:s3:::company-dev-s3-cicd-xxx/codebuild/myproject/artifacts/f4cc70cc-0f38-4cd3-b839-5464bf6b8ed5/codebuild_artifacts.zip"
    artifacts_s3=`echo ${artifacts_location} | awk -F: '{print $6}'`;
    artifacts_s3_bucket=`echo ${artifacts_s3} | cut -d '/' -f 1`;
    artifacts_s3_key=`echo ${artifacts_s3} | cut -d '/' -f 2-`;

    mkdir .codebuild_artifacts;
    cd .codebuild_artifacts;

    ## Fetch artifact files in current dir
    CODEBUILD_ARTIFACTS_PACKAGING=${CODEBUILD_ARTIFACTS_PACKAGING:-"ZIP"}

    if [ "${CODEBUILD_ARTIFACTS_PACKAGING}" = "NONE" ]; then
      aws s3 sync s3://${artifacts_s3_bucket}/${artifacts_s3_key}/ .;
    elif [ "${ARTIFACTS_PACKAGING}" = "ZIP" ] ; then
      if `aws s3 cp --quiet s3://${artifacts_s3_bucket}/${artifacts_s3_key} .` ; then
        artifacts_name=`basename $artifacts_s3_key`;
        unzip $artifacts_name;
        echo "Fetched artifacts from ${artifacts_s3_bucket}/${artifacts_s3_key}";
      else
        echo "Failed fetching artifacts from ${artifacts_s3_bucket}/${artifacts_s3_key}";
      fi
    else
      echo "Unsupported CODEBUILD_ARTIFACTS_PACKAGING value"
    fi

  fi

fi
